import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support, confusion_matrix, precision_score, recall_score, roc_auc_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.decomposition import PCA
from joblib import dump

# Load dataset
df = pd.read_csv("Insurance Dataset.csv")

# Data preprocessing
df.columns
df.head()
df.isnull().sum()
df.shape
df.nunique()
df.dtypes
columns_to_drop = ['Payment_Typology']
df.drop(columns_to_drop, axis=1, inplace=True)
df['baby'] = df['Weight_baby'].apply(lambda x: 1 if x > 0 else 0)
columns_to_drop = ['Weight_baby']
df.drop(columns_to_drop, axis=1, inplace=True)
df['Hospital Id'].unique()
df['Hospital Id'] = df['Hospital Id'].astype(object)
df.loc[df['Days_spend_hsptl'] == '120 +', 'Days_spend_hsptl'] = 120
df['Days_spend_hsptl'] = df ['Days_spend_hsptl'].astype(np.float64)
df['Code_illness'] = df['Code_illness'].astype(np.int64)
df.dtypes

# Prepare features and target variable
X = df.copy()
y = X.pop('Result')
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0, stratify=y)

# Impute missing values
mask = X_train.dtypes == float
float_cols = X_train.columns[mask]
imputer_continuous = SimpleImputer(strategy='median')
imputer_continuous.fit(X_train[float_cols])
X_train[float_cols] = imputer_continuous.transform(X_train[float_cols])
X_test[float_cols] = imputer_continuous.transform(X_test[float_cols])

mask = X_train.dtypes == object
object_cols = X_train.columns[mask]
imputer_categorical = SimpleImputer(strategy='most_frequent')
imputer_categorical.fit(X_train[object_cols])
X_train[object_cols] = imputer_categorical.transform(X_train[object_cols])
X_test[object_cols] = imputer_categorical.transform(X_test[object_cols])

X_train['Hospital Id'] = X_train['Hospital Id'].astype(np.int64)
X_test['Hospital Id'] = X_test['Hospital Id'].astype(np.int64)
X_train['Mortality risk'] = X_train['Mortality risk'].astype(np.int64)
X_test['Mortality risk'] = X_test['Mortality risk'].astype(np.int64)

# Standardize numerical features
ss = StandardScaler()
mask_numeric = X_train.dtypes == float
numeric_cols = X_train.columns[mask_numeric]
numeric_cols = numeric_cols.tolist()
X_train[numeric_cols] = ss.fit_transform(X_train[numeric_cols])
X_test[numeric_cols] = ss.transform(X_test[numeric_cols])

# One-hot encoding
mask = X_train.dtypes == object
object_cols = X_train.columns[mask]
one_hot = ColumnTransformer(transformers=[("one_hot", OneHotEncoder(), object_cols)], remainder="passthrough")
X_train = one_hot.fit_transform(X_train)
X_test = one_hot.transform(X_test)
names = one_hot.get_feature_names_out()
column_names = [name[name.find("") + 1:] for name in [name[name.find("_") + 2:] for name in names]]
X_train = X_train.toarray()
X_train = pd.DataFrame(data=X_train, columns=column_names)
X_test = X_test.toarray()
X_test = pd.DataFrame(data=X_test, columns=column_names)

# Apply PCA
pca = PCA()
pca.fit(X_train)
X_train_hat = pca.transform(X_train)
print(X_train_hat.shape)
plt.plot(pca.explained_variance_ratio_)
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.title("Component-wise variance and cumulative explained variance")
n_components = np.argmax(np.cumsum(pca.explained_variance_ratio_) >= 0.9999) + 1
print("Number of components required to explain 99.99% variance:", n_components)
X_test_hat = pca.transform(X_test)
print(X_test_hat.shape)

# Prepare PCA-transformed data
X_train_hat_PCA = pd.DataFrame(columns=[f'Projection on Component {i + 1}' for i in range(len(X_train.columns))], data=X_train_hat)
X_test_hat_PCA = pd.DataFrame(columns=[f'Projection on Component {i + 1}' for i in range(len(X_train.columns))], data=X_test_hat)
N = 4
X_train_hat_PCA = X_train_hat_PCA.iloc[:, :N]
X_test_hat_PCA = X_test_hat_PCA.iloc[:, :N]

# Train and evaluate Decision Tree model
dt = DecisionTreeClassifier(criterion='entropy', max_depth=5, random_state=1)
dt.fit(X_train_hat_PCA, y_train)
y_preds_dt = dt.predict(X_test_hat_PCA)

# Evaluate metrics
def evaluate_metrics(yt, yp):
    accuracy = accuracy_score(yt, yp)
    precision, recall, fbeta, support = precision_recall_fscore_support(yt, yp, beta=2, pos_label=1, average='binary')
    auc = roc_auc_score(yt, yp)

evaluate_metrics(y_test, y_preds_dt)

# Save the Decision Tree model and preprocessing components
dump(dt, 'decision_tree_model.joblib')
dump(one_hot, 'one_hot_encoder.joblib')
dump(pca, 'pca.joblib')
dump(ss, 'standardscaler.joblib')
